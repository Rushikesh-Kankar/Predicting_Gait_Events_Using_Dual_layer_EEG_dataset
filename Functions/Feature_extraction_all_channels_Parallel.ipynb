{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7160d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#trailcell\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import pywt\n",
    "\n",
    "\n",
    "# from scipy.stats import skew, kurtosis\n",
    "# from mne import Epochs\n",
    "# from scipy.signal import correlate\n",
    "# from joblib import Parallel, delayed\n",
    "# from scipy.stats import entropy\n",
    "# from pyentrp import entropy as ent\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# # Define frequency bands\n",
    "# bands = {'delta': (0.5, 4),\n",
    "#          'theta': (4, 8),\n",
    "#          'alpha': (8, 13),\n",
    "#          'beta': (13, 30)}\n",
    "\n",
    "# def extract_psd_features(epoch):\n",
    "#     freqs = epoch.compute_psd().freqs\n",
    "#     band_inds = dict()\n",
    "#     for band, (fmin, fmax) in bands.items():\n",
    "#         band_inds[band] = np.where((freqs >= fmin) & (freqs <= fmax))\n",
    "\n",
    "#     psd_data = epoch.compute_psd().get_data()[0]\n",
    "#     band_power = dict()\n",
    "#     for band, inds in band_inds.items():\n",
    "#         band_power[band] = np.mean(psd_data[:, inds], axis=1)\n",
    "\n",
    "#     flat_power = np.concatenate([band_power[band].flatten() for band in bands], axis=0)\n",
    "#     return flat_power\n",
    "\n",
    "# def extract_time_domain_features(epoch):\n",
    "#     features = []\n",
    "#     data = epoch.get_data()\n",
    "#     for i in range(data.shape[1]):\n",
    "#         channel_data = data[:, i].squeeze()\n",
    "#         features.append(np.mean(channel_data))\n",
    "#         features.append(np.std(channel_data))\n",
    "#         features.append(skew(channel_data))\n",
    "#         features.append(kurtosis(channel_data))\n",
    "#     return features\n",
    "\n",
    "# def extract_wavelet_features(epoch):\n",
    "#     coeffs = []\n",
    "#     data = epoch.get_data()\n",
    "#     for i in range(data.shape[1]):\n",
    "#         channel_data = data[:, i].squeeze()\n",
    "#         cA, cD = pywt.dwt(channel_data, 'db4')  # Decompose signal using Daubechies 4 wavelet\n",
    "#         coeffs.append(cA.mean())\n",
    "#         coeffs.append(cA.std())\n",
    "#         coeffs.append(np.abs(cD).mean())  # Take absolute value of cD coefficients to account for negative values\n",
    "#         coeffs.append(np.abs(cD).std())\n",
    "#         coeffs.append(skew(np.abs(cD).flatten()))  # Skewness of cD coefficients\n",
    "#         coeffs.append(kurtosis(np.abs(cD).flatten()))  # Kurtosis of cD coefficients\n",
    "#         coeffs.append(np.sum(np.square(cA)))  # Energy of wavelet coefficients\n",
    "#         coeffs.append(np.sum(np.square(cD)))  # Energy of detail coefficients\n",
    "#         coeffs.append(-np.sum(np.square(cA) * np.log(np.square(cA))))  # Wavelet entropy of approximation coefficients\n",
    "#         coeffs.append(-np.sum(np.square(cD) * np.log(np.square(cD))))  # Wavelet entropy of detail coefficients\n",
    "#     return coeffs\n",
    "\n",
    "\n",
    "# def extract_autocorrelation_features(epoch, max_lag=None):\n",
    "#     features = []\n",
    "#     data = epoch.get_data()\n",
    "#     for i in range(data.shape[1]):\n",
    "#         channel_data = data[:, i].squeeze()\n",
    "#         autocorr = correlate(channel_data, channel_data, mode='full')\n",
    "#         autocorr = autocorr[autocorr.size // 2:]  # Keep only the positive lags\n",
    "\n",
    "#         if max_lag is not None:\n",
    "#             autocorr = autocorr[:max_lag]\n",
    "\n",
    "#         max_lag_at_max_autocorr = np.argmax(autocorr)  # Find the time lag with maximum autocorrelation\n",
    "#         features.append(max_lag_at_max_autocorr)\n",
    "\n",
    "#     return features\n",
    "\n",
    "# def shannon_entropy(channel_data):\n",
    "#     p_data = np.histogram(channel_data, bins=64, density=True)[0]\n",
    "#     return entropy(p_data)\n",
    "\n",
    "# def approximate_entropy(channel_data, order=2, metric='chebyshev'):\n",
    "#     return ent.approximate_entropy(channel_data, order, metric)\n",
    "\n",
    "# def sample_entropy(channel_data, order=2, metric='chebyshev'):\n",
    "#     return ent.sample_entropy(channel_data, order, metric)\n",
    "\n",
    "# def permutation_entropy(channel_data, order=3, delay=1, normalize=False):\n",
    "#     return ent.permutation_entropy(channel_data, order, delay, normalize)\n",
    "\n",
    "# def extract_entropy_features(epoch):\n",
    "#     features = []\n",
    "#     data = epoch.get_data()\n",
    "#     for i in range(data.shape[1]):\n",
    "#         channel_data = data[:, i].squeeze()\n",
    "#         features.append(shannon_entropy(channel_data))\n",
    "#         features.append(approximate_entropy(channel_data))\n",
    "#         features.append(sample_entropy(channel_data))\n",
    "#         features.append(permutation_entropy(channel_data))\n",
    "\n",
    "#     return features\n",
    "\n",
    "\n",
    "# # def extract_combined_features(all_data):\n",
    "# #     # Initialize an empty DataFrame to store the combined feature data\n",
    "# #     combined_feature_df = pd.DataFrame()\n",
    "\n",
    "# #     # Define a function to extract features for a single data object\n",
    "# #     def extract_features_for_single_data(data):\n",
    "# #         psd_features = extract_psd_features(data)\n",
    "# #         time_domain_features = extract_time_domain_features(data)\n",
    "# #         wavelet_features = extract_wavelet_features(data)\n",
    "# #         #autocorrelation_features = extract_autocorrelation_features(data)\n",
    "\n",
    "# #         # Combine all features into a single list\n",
    "# #         #combined_features = np.hstack([psd_features, time_domain_features, wavelet_features, autocorrelation_features])\n",
    "        \n",
    "# #         entropy_features = extract_entropy_features(data)\n",
    "\n",
    "# #         # Combine all features into a single list\n",
    "# #         #combined_features = np.hstack([psd_features, time_domain_features, wavelet_features, autocorrelation_features, entropy_features])\n",
    "# #         #return combined_features\n",
    "        \n",
    "# #         # Combine all features into a single list\n",
    "# #         combined_features = np.hstack([psd_features, time_domain_features, wavelet_features])\n",
    "# #         return combined_features\n",
    "    \n",
    "# #     # Use Joblib to parallelize the feature extraction\n",
    "# #     combined_features_list = Parallel(n_jobs=-1)(delayed(extract_features_for_single_data)(all_data[i]) for i in range(len(all_data)))\n",
    "\n",
    "# #     # Append the combined features as rows in the combined_feature_df dataframe\n",
    "# #     for combined_features in combined_features_list:\n",
    "# #         combined_feature_df = combined_feature_df.append(pd.Series(combined_features), ignore_index=True)\n",
    "\n",
    "# #     return combined_feature_df\n",
    "\n",
    "\n",
    "# def extract_combined_features(all_data):\n",
    "#     # Initialize an empty DataFrame to store the combined feature data\n",
    "#     combined_feature_df = pd.DataFrame()\n",
    "\n",
    "#     # Define a function to extract features for a single data object\n",
    "#     def extract_features_for_single_data(data):\n",
    "#         psd_features = extract_psd_features(data)\n",
    "#         time_domain_features = extract_time_domain_features(data)\n",
    "#         wavelet_features = extract_wavelet_features(data)\n",
    "#         entropy_features = extract_entropy_features(data)\n",
    "\n",
    "#         # Combine all features into a single list\n",
    "#         combined_features = np.hstack([psd_features, time_domain_features, wavelet_features])\n",
    "\n",
    "#         return combined_features\n",
    "    \n",
    "#     # Use Joblib to parallelize the feature extraction and tqdm to show the progress\n",
    "#     combined_features_list = Parallel(n_jobs=-1)(delayed(extract_features_for_single_data)(all_data[i]) for i in tqdm(range(len(all_data)), desc=\"Extracting combined features\"))\n",
    "\n",
    "#     # Append the combined features as rows in the combined_feature_df dataframe\n",
    "#     for combined_features in combined_features_list:\n",
    "#         combined_feature_df = combined_feature_df.append(pd.Series(combined_features), ignore_index=True)\n",
    "\n",
    "#     return combined_feature_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f53984cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pywt\n",
    "from scipy.stats import skew, kurtosis\n",
    "from mne import Epochs\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define frequency bands\n",
    "bands = {'delta': (0.5, 4),\n",
    "         'theta': (4, 8),\n",
    "         'alpha': (8, 13),\n",
    "         'beta': (13, 30)}\n",
    "\n",
    "def extract_psd_features(epoch):\n",
    "    freqs = epoch.compute_psd().freqs\n",
    "    band_inds = dict()\n",
    "    for band, (fmin, fmax) in bands.items():\n",
    "        band_inds[band] = np.where((freqs >= fmin) & (freqs <= fmax))\n",
    "\n",
    "    psd_data = epoch.compute_psd().get_data()[0]\n",
    "    band_power = dict()\n",
    "    for band, inds in band_inds.items():\n",
    "        band_power[band] = np.mean(psd_data[:, inds], axis=1)\n",
    "\n",
    "    flat_power = np.concatenate([band_power[band].flatten() for band in bands], axis=0)\n",
    "    return flat_power\n",
    "\n",
    "def extract_time_domain_features(epoch):\n",
    "    features = []\n",
    "    data = epoch.get_data()\n",
    "    for i in range(data.shape[1]):\n",
    "        channel_data = data[:, i].squeeze()\n",
    "        features.append(np.mean(channel_data))\n",
    "        features.append(np.std(channel_data))\n",
    "        features.append(skew(channel_data))\n",
    "        features.append(kurtosis(channel_data))\n",
    "    return features\n",
    "\n",
    "def extract_wavelet_features(epoch):\n",
    "    coeffs = []\n",
    "    data = epoch.get_data()\n",
    "    for i in range(data.shape[1]):\n",
    "        channel_data = data[:, i].squeeze()\n",
    "        cA, cD = pywt.dwt(channel_data, 'db4')  # Decompose signal using Daubechies 4 wavelet\n",
    "        coeffs.append(cA.mean())\n",
    "        coeffs.append(cA.std())\n",
    "        coeffs.append(np.abs(cD).mean())  # Take absolute value of cD coefficients to account for negative values\n",
    "        coeffs.append(np.abs(cD).std())\n",
    "        coeffs.append(skew(np.abs(cD).flatten()))  # Skewness of cD coefficients\n",
    "        coeffs.append(kurtosis(np.abs(cD).flatten()))  # Kurtosis of cD coefficients\n",
    "        coeffs.append(np.sum(np.square(cA)))  # Energy of wavelet coefficients\n",
    "        coeffs.append(np.sum(np.square(cD)))  # Energy of detail coefficients\n",
    "        coeffs.append(-np.sum(np.square(cA) * np.log(np.square(cA))))  # Wavelet entropy of approximation coefficients\n",
    "        coeffs.append(-np.sum(np.square(cD) * np.log(np.square(cD))))  # Wavelet entropy of detail coefficients\n",
    "    return coeffs\n",
    "\n",
    "def extract_combined_features(all_data):\n",
    "    # Initialize an empty DataFrame to store the combined feature data\n",
    "    combined_feature_df = pd.DataFrame()\n",
    "\n",
    "    # Define a function to extract features for a single data object\n",
    "    def extract_features_for_single_data(data):\n",
    "        psd_features = extract_psd_features(data)\n",
    "        time_domain_features = extract_time_domain_features(data)\n",
    "        wavelet_features = extract_wavelet_features(data)\n",
    "\n",
    "        # Combine all features into a single list\n",
    "        combined_features = np.hstack([psd_features, time_domain_features, wavelet_features])\n",
    "        return combined_features\n",
    "\n",
    "    # Use Joblib to parallelize the feature extraction\n",
    "    #combined_features_list = Parallel(n_jobs=-1)(delayed(extract_features_for_single_data)(all_data[i]) for i in range(len(all_data)))\n",
    "       # Use Joblib to parallelize the feature extraction and tqdm to show the progress\n",
    "    combined_features_list = Parallel(n_jobs=-1)(delayed(extract_features_for_single_data)(all_data[i]) for i in tqdm(range(len(all_data)), desc=\"Extracting combined features\"))\n",
    "\n",
    "    # Append the combined features as rows in the combined_feature_df dataframe\n",
    "    for combined_features in combined_features_list:\n",
    "        combined_feature_df = combined_feature_df.append(pd.Series(combined_features), ignore_index=True)\n",
    "\n",
    "    return combined_feature_df\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
